---
title: S04
dimension: strategic
tags: [design,objectives,drivers]
nav_order: 2.14
description: Objectives

requirement: |
  The solution design **SHOULD** be able to meet the stated user needs and overall business objectives/drivers.

todos:
  - what are examples of what those needs look like
  - how to critique those needs
  - where to find the current overall business drivers?

more_info: |
  Intent:
    Demonstrate clear alignment between articulated user needs, business
    objectives and architectural design choices.

  Needs & objectives linkage:
    - User need -> Capability -> Feature / Epic -> Architectural Component
    - Each objective has measurable success metric & baseline
    - Architectural decisions reference objective impact (positive / trade-off)

  Critique checklist:
    - Is the need phrased from user perspective (not internal system desire)?
    - Does objective have time-bound target & metric?
    - Are conflicting objectives (speed vs cost) explicitly balanced?
    - Is there traceable evidence of user research / data?

  Evidence artefacts:
    - Objective to capability trace matrix
    - Decision records annotating objective influence
    - Metrics dashboard showing progress towards targets

  Pitfalls:
    - Listing vague aspirations ("improve experience") without metric
    - Retrofitting objectives after design locked
    - Ignoring negative impact or opportunity cost of choices

examples: 
    - title: Objective Trace Table
      content: |
        Objective -> Metrics -> Related Epics -> ADR IDs.
    - title: User Need Statement
      content: |
        "As a clinician I need consolidated alerts so I can act without switching systems."
    - title: Dashboard Metric Tile
      content: |
        Shows progress: mean onboarding time 25% reduced vs 30% target.

technology:
    - title: Product Analytics Platform
      content: |
        Captures user behaviour & outcome metrics.
    - title: Requirements Traceability Matrix Tool
      content: |
        Links needs -> epics -> decisions -> components.
    - title: User Research Repository
      content: |
        Stores evidence underpinning needs & assumptions.

further_reading:
    - title: Lean UX Principles
      content: Outcome-focused design & validation loops.
      url: https://www.oreilly.com/
    - title: GDS Service Standard (User Needs)
      content: Government approach to articulating and validating needs.
      url: https://www.gov.uk/service-manual/user-research/start-by-learning-user-needs

assessment_guidance: |
  Assessment focus:
    Establish traceable, measurable linkage from validated user needs to business objectives and architectural choices.

  Steps:
    1. Sample 5 user need statements: ensure user-perspective phrasing, evidence reference (research session / data) and no embedded solution.
    2. For 2 business objectives: confirm SMART-style metric (baseline + target + timeframe) and linkage to needs & epics.
    3. Pick 2 architecture decisions: identify explicit objective impact section (positive, trade-off) present.
    4. Inspect outcome dashboard: verify metrics align exactly with objective identifiers & show trend.
    5. Evaluate challenge process: evidence of retired or refined needs after validation cycles?

  Evidence:
    - Need statements with evidence references
    - Objective trace row (Objective → Need(s) → Epic(s) → ADR(s))
    - Metric tile screenshot (baseline, current, target)
    - Example of retired need / pivot rationale

  Red flags:
    - Objectives lacking baseline or time horizon
    - Needs duplicated / overlapping without consolidation note
    - Decisions referencing “business priorities” generically (no ID)
    - Metrics measuring outputs (story count) not outcomes (user value)

  Maturity signals:
    - Automated trace matrix generation from structured metadata
    - Regular pruning of stale / invalid needs (tracked change log)
    - Hypothesis statements paired with each new objective

  Quick improvements:
    - Add ‘Evidence’ field to need template
    - Introduce objective_id tagging across ADRs & epics
    - Run quarterly objective-need traceability audit script

assessment_examples:
  "0":
    - example: No formal objectives; vague aspirations and undocumented user needs.
    - example: Architecture decisions lack any reference to objectives or needs.
  "1":
    - example: Basic objective list exists without measurable targets or baselines.
    - example: User needs recorded but many are solution statements or duplicate.
  "2":
    - example: Some objectives have baseline & target; partial trace to epics established manually.
    - example: A few ADRs reference objective IDs inconsistently.
  "3":
    - example: Full trace (Need → Objective → Epic → ADR) maintained; metrics dashboard reflects current progress.
    - example: Retired / merged needs logged with rationale after validation.
  "4":
    - example: Automated generation of trace matrix; objective impact section mandatory in ADR template.
    - example: Periodic audit removes stale needs; hypothesis → experiment outcomes captured.
  "5":
    - example: Live analytics feed updates objective metrics; unmet objective thresholds trigger automatic review tasks.
    - example: Machine-readable objective metadata drives dashboard & validation lint in CI.


---
