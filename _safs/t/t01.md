---
title: T01
dimension: choices
tags: [technology,choices,radar,tech radar,decisions,innovation,lifecycle,governance,adoption,emerging]
nav_order: 2.41

description: Technology Radar

requirement: |
  Technology choices **SHOULD** be made in line with the NHS England Technology Radar, corporate direction and wider industry trends using the associated processes to support decision making. (Note there are overlaps with the Engineering red lines; ensure a consistent response and do not repeat assessments)

more_info: |
  Intent:
    Align technology adoption with organisational direction, maturity posture and
    ecosystem evolutionâ€”avoiding fragmentation and costly divergence.

  Radar usage:
    - Map candidate tech to radar ring (Adopt / Trial / Assess / Hold)
    - Justify movement between rings via evaluation outcomes
    - Flag exceptions (using non-adopt tech) with rationale & exit plan

  Evaluation dimensions:
    - Strategic fit & principle alignment
    - Ecosystem support (skills, tooling, community, vendor viability)
    - Operability (observability, upgrade path, resilience story)
    - Security & compliance posture
    - Total cost of ownership & lock-in profile

  Pitfalls:
    - Cargo-cult adoption based on hype curves
    - Over-customising platform tech (hard to upgrade)
    - Hidden dependency sprawl (plugins, proprietary extensions)

examples: 
    - title: Radar Entry Justification
      content: |
        Technology X moved to Trial: pilot success criteria defined (latency < 50ms, admin overhead < 1 FTE).
    - title: Exception Record
      content: |
        Using "Hold" tech Y due to existing contractual dependency; plan to retire by Q3.
    - title: Comparative Evaluation Table
      content: |
        Columns: Candidate, Operability Score, Security Score, Cost, Decision.

technology:
    - title: Tech Radar Repository / Tool
      content: |
        Stores ring assignments & history (e.g. YAML + static site).
    - title: Evaluation Template (Markdown)
      content: |
        Standardises evidence capture & scoring.
    - title: Skills Inventory Dashboard
      content: |
        Correlates workforce capability with adoption risk.

further_reading:
    - title: ThoughtWorks Technology Radar
      content: External perspective on emerging tech movement.
      url: https://www.thoughtworks.com/radar
    - title: Gartner Hype Cycle (Context)
      content: Interpreting maturity vs hype.
      url: https://www.gartner.com/

assessment_guidance: |
  Assessment focus:
    Validate disciplined technology adoption aligned to radar, with managed exceptions and lifecycle review.

  Steps:
    1. Inspect radar entries for newly adopted tech: confirm rationale & success criteria defined pre-adoption.
    2. Sample one tech moving ring recently: check evidence of evaluation vs criteria (not opinion only).
    3. Examine exception list (use of Hold technologies): verify exit or mitigation plan.
    4. Cross-reference skill inventory: ensure adoption not outpacing capability (training / hiring plan present).
    5. Review upgrade / patch cadence for adopted tech (security + end-of-life awareness).

  Evidence:
    - Radar change log entry
    - Evaluation template filled sample
    - Exception record with target retirement
    - Skills readiness matrix

  Red flags:
    - Multiple unreviewed trial technologies persisting > agreed trial window
    - Skill gaps causing operational incidents
    - Silent drift (libraries / platforms outdated vs policy target)

  Maturity signals:
    - Automated radar site build with historical ring movement diff
    - Adoption success metrics (stability, performance) tracked post-trial
    - Internal tech retirement backlog managed proactively

  Quick improvements:
    - Add adoption review checklist gating ring promotion
    - Tag repositories with technology IDs for inventory queries
    - Quarterly radar hygiene audit script

assessment_examples:
  "0":
    - example: Technology choices ad-hoc; radar not referenced.
    - example: No record of rationale or evaluation.
  "1":
    - example: Radar consulted occasionally; ring assignments undocumented; success criteria absent.
    - example: Exceptions (Hold tech) untracked.
  "2":
    - example: Evaluation templates used for some adoptions; basic success metrics outlined.
    - example: Trial technologies linger past intended window without review.
  "3":
    - example: All new adoptions have pre-defined success criteria & post-trial evaluation; ring movements logged.
    - example: Exception list maintained with exit plans.
  "4":
    - example: Automated diff of ring movements; adoption success metrics feed dashboards guiding decisions.
    - example: Skill inventory integrated; training backlog aligns to adoption plan.
  "5":
    - example: Predictive analytics highlight tech needing retirement or upgrade; automated nudges for overdue trials.
    - example: Technology portfolio KPIs (time-to-adopt, retirement lead time) optimised & improving.






---
