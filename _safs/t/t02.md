---
title: T02
dimension: choices
tags: [choices,engineering]
nav_order: 2.42

description: Non Functional Needs

requirement: |
  Technology choices **SHOULD** be appropriate to the problem and non-functional needs. I.e. we are as equally aware of over-engineering as we are of under-engineering.

more_info: |
  Goal:
    Select technology with fidelity to actual non-functional needs—balancing
    simplicity and necessary capability (avoid gold-plating & undue fragility).

  NFR alignment steps:
    1. Quantify critical NFRs (latency p95, availability %, throughput, RPO/RTO)
    2. Characterise workload patterns (burst, steady, batch, event-driven)
    3. Evaluate candidate tech against measurable NFR thresholds
    4. Run representative benchmarks / proof scenarios
    5. Document trade-offs & revisit triggers

  Over-engineering smells:
    - Complex distributed store for low-volume config data
    - Multi-cluster mesh when single cluster suffices
    - Premature sharding / partitioning

  Under-engineering smells:
    - Single instance stateful component w/o failover
    - No capacity threshold alerts or autoscaling plan
    - Ignoring data retention & archival policy

examples: 
    - title: NFR Traceability Table
      content: |
        Latency (p95 < 120ms) -> Candidate A passes, Candidate B fails benchmark.
    - title: Benchmark Result Snapshot
      content: |
        Load test graph comparing error rate at rising concurrency levels.
    - title: Right-Sizing Decision ADR
      content: |
        Recorded choice to avoid microservices split until sustained TPS > X.

technology:
    - title: Load / Stress Test Tool (k6 / Gatling)
      content: |
        Generates realistic non-functional workload.
    - title: Chaos Engineering Tool
      content: |
        Validates resilience claims (failure injection).
    - title: Capacity Planning Spreadsheet / Model
      content: |
        Projects resource vs volume growth.

further_reading:
    - title: Google SRE Workbook (NFRs)
      content: Approaches to defining & measuring reliability.
      url: https://sre.google/books/
    - title: Chaos Engineering Principles
      content: Intentional failure to validate assumptions.
      url: https://principlesofchaos.org/

assessment_guidance: |
  Assessment focus:
    Ensure technology choices are right-sized to quantified NFRs; detect over/under engineering risk early.

  Steps:
    1. Review NFR specification: confirm numeric targets (latency, throughput, availability, RPO/RTO) exist & trace to business impact.
    2. Inspect benchmark results comparing candidate technologies vs targets; verify realistic workload modelling.
    3. Evaluate capacity model for chosen tech: scaling limits & alert thresholds defined?
    4. Sample one potential over-engineering and one under-engineering smell from code/infra; check recorded decision & revisit trigger.
    5. Confirm chaos / resilience tests or load tests exist validating assumptions.

  Evidence:
    - NFR table extract
    - Benchmark comparison chart
    - Capacity / scaling thresholds doc
    - Chaos or load test report

  Red flags:
    - Vague NFRs (“fast”, “highly available”) without numbers
    - Benchmark purely synthetic ignoring real payload mix
    - Scaling assumptions unvalidated by experimentation

  Maturity signals:
    - Automated performance regression tests in pipeline
    - Periodic right-sizing review results with cost/perf deltas
    - Revisit triggers resulting in simplification when scale absent

  Quick improvements:
    - Add performance budget enforcement script
    - Introduce baseline chaos experiment (dependency latency injection)
    - Maintain NFR→Tech choice rationale index

assessment_examples:
  "0":
    - example: Technology picked without quantified NFRs; assumptions implicit.
    - example: Over/under engineering issues discovered only after incidents.
  "1":
    - example: Some NFRs drafted qualitatively; preliminary benchmarks superficial.
    - example: Capacity / scaling plan undocumented.
  "2":
    - example: NFRs quantified; benchmark run for main candidate; limited chaos testing.
    - example: Revisit trigger defined but not tracked.
  "3":
    - example: Benchmarks & load tests aligned to realistic scenarios; capacity thresholds & alerts configured.
    - example: Periodic review adjusts architecture (e.g., defers unnecessary sharding).
  "4":
    - example: Automated performance regression suite gating merges; chaos tests cover critical failure modes.
    - example: Right-sizing review outcomes reduce cost / complexity measurably.
  "5":
    - example: Continuous performance telemetry forecasts scaling inflection; proactive simplification or upgrade executed.
    - example: Self-service dashboard ties NFR targets, benchmark history & tech choice rationales.



---
