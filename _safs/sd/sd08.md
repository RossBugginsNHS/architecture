---
title: SD08
dimension: solution
tags: [patterns]
nav_order: 2.38

description: Patterns

requirement: |
  We **SHOULD** select the correct patterns for the use case, and any trade-offs justified and agreed.

more_info: |
  Goal:
    Deliberately select patterns (integration, data, deployment, resilience) that
    fit the problem characteristics; justify trade-offs & operational impact.

  Pattern selection workflow:
    1. Characterise use case (latency, consistency, scale, coupling tolerance)
    2. List candidate patterns (e.g. event-driven, request/response, CQRS)
    3. Evaluate against criteria (simplicity, resilience, cost, skill fit)
    4. Capture rationale & rejected alternatives
    5. Define success metrics (e.g. p95 latency, failure isolation scope)

  Common categories:
    - Integration (synchronous REST, gRPC, event streaming)
    - Data (CRUD, event sourcing, CQRS, replication)
    - Resilience (circuit breaker, bulkhead, backoff)
    - Deployment (blue/green, canary, rolling)
    - Caching (read-through, write-behind, edge cache)

  Pitfalls:
    - Pattern enthusiasm (introducing complexity w/o value)
    - Copying patterns from unrelated scale domain
    - Neglecting exit / simplification path if scale assumptions fail

examples: 
    - title: Pattern Evaluation Matrix
      content: |
        Rows: Pattern; Columns: Criteria (score + note), Selected? Y/N.
    - title: Event-Driven vs Polling ADR Extract
      content: |
        Trade-off summary leading to event streaming adoption.
    - title: Latency Objective Definition
      content: |
        Document specifying p95 & p99 thresholds guiding pattern pick.

technology:
    - title: Architecture Pattern Catalogue
      content: |
        Internal curated markdown collection with decision heuristics.
    - title: Performance Test Harness
      content: |
        Benchmarks candidate approaches under projected load.
    - title: Chaos / Resilience Testing Tool
      content: |
        Validates pattern behaviour under failure scenarios.

further_reading:
    - title: Enterprise Integration Patterns
      content: Canonical integration pattern references.
      url: https://www.enterpriseintegrationpatterns.com/
    - title: Release It!
      content: Resilience & stability pattern exploration.
      url: https://pragprog.com/titles/mnee2/release-it-second-edition/

assessment_guidance: |
  Assessment focus:
    Determine if pattern choices are evidence-based with explicit trade-offs, metrics and revisit triggers.

  Steps:
    1. Review pattern evaluation matrix: confirm rejected options list with concise rationale.
    2. Sample one selected pattern (e.g. CQRS, event sourcing): verify success metrics defined & currently monitored.
    3. Inspect resilience pattern instrumentation (circuit breaker stats, retry metrics) for health.
    4. Check any complexity countermeasures (simplification plan) if advanced patterns adopted pre-scale.
    5. Validate revisit trigger existence (e.g. threshold beyond which alternative pattern considered).

  Evidence:
    - Evaluation matrix snippet
    - Monitoring panel for pattern KPIs
    - Circuit breaker / retry dashboard excerpt
    - Revisit trigger log entry

  Red flags:
    - Pattern adoption without measurable success definition
    - High operational toil owning bespoke complex pattern glue
    - Rejected options undocumented (decision bias risk)

  Maturity signals:
    - Fitness functions auto-verifying pattern constraints (idempotency, latency)
    - Periodic pattern portfolio review outcomes logged
    - Decommission plan for patterns no longer justified

  Quick improvements:
    - Add pattern success metrics to observability dashboards
    - Create pattern adoption checklist with gating questions
    - Capture pattern revisit outcomes in a consolidated log

---
