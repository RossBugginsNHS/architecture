---
title: SD12
dimension: solution
tags: [frameworks,well architected]
nav_order: 2.393

description: Well Architected Frameworks

requirement: |
  The relevant cloud "Well Architected Framework" **SHOULD** be followed, and the solutions assessed against it. AWS | Azure (Note there are overlaps with the Engineering Software Quality Framework; ensure a consistent response and do not repeat assessments)

more_info: |
  Goal:
    Systematically evaluate solutions against cloud Well-Architected pillars to
    surface improvement opportunities early and track remediation.

  Assessment cadence:
    - Baseline at initial architecture approval
    - Reassess per major release / significant scale change
    - Light-touch quarterly checkpoint for high-criticality services

  Pillar examples (AWS / Azure mapping):
    - Operational Excellence / Operational Excellence
    - Security / Security
    - Reliability / Reliability
    - Performance Efficiency / Performance Efficiency
    - Cost Optimization / Cost Management
    - Sustainability (where available)

  Output structure:
    - Pillar -> Question / Check
    - Current State Rating (e.g. 1–5 or Red/Amber/Green)
    - Risk / Impact Statement
    - Recommended Action (owner, target date)
    - Status Tracking

  Pitfalls:
    - One-off assessment (no iterative improvement)
    - Generic action wording (not implementable / measurable)
    - Ignoring sustainability / cost trade-offs in performance tuning

examples: 
    - title: Pillar Assessment Table Row
      content: |
        Reliability - Data Backup Strategy: Rating Amber, Risk: RPO > target,
        Action: implement incremental backup & test restore (Q1).
    - title: Improvement Backlog Entry
      content: |
        Ticket referencing Well-Architected finding with acceptance criteria.
    - title: Trend Chart
      content: |
        Visual showing security pillar scores improving over 3 assessments.

technology:
    - title: Cloud Well-Architected Tooling
      content: |
        Platform-native questionnaires & remediation tracking.
    - title: Infrastructure as Code Scanners
      content: |
        Detect drift & non-compliant configurations.
    - title: Cost / Performance Monitoring Suite
      content: |
        Correlates optimisation actions with metric changes.

further_reading:
    - title: AWS Well-Architected Framework
      content: Core pillar guidance & lenses.
      url: https://aws.amazon.com/architecture/well-architected/
    - title: Azure Well-Architected Framework
      content: Microsoft cloud pillar guidance.
      url: https://learn.microsoft.com/azure/architecture/framework/

assessment_guidance: |
  Assessment focus:
    Determine if Well-Architected assessments drive measurable improvement and are integrated into delivery cadence.

  Steps:
    1. Review last two assessment reports: identify delta—did recommendations translate into tracked backlog items with owners?
    2. Sample 3 open remediation actions: check status, target date and progress evidence.
    3. Compare pillar scores trend (e.g. Security, Cost) across assessments—look for stagnation without rationale.
    4. Validate automated checks (IaC scanning, cost anomaly detection) align to identified risks in reports.
    5. Inspect sustainability or cost optimisation recommendations—trace at least one to implemented change & metric shift.

  Evidence:
    - Assessment report excerpts (before/after table)
    - Remediation backlog snapshot
    - Pillar score trend chart
    - Automation scan output mapping to report findings

  Red flags:
    - Repeated identical recommendations unclosed over multiple cycles
    - High-risk findings without risk acceptance record or action plan
    - Assessments treated as annual checkbox (long intervals)

  Maturity signals:
    - Rolling remediation burn-down visible to stakeholders
    - Pillar score improvement correlating with operational metrics (e.g. reduced incidents)
    - Lightweight interim self-assessments feeding main reviews

  Quick improvements:
    - Introduce remediation SLA categories (High < 45 days, Medium < 90)
    - Automate pillar score calculation from tagged evidence YAML
    - Add quarterly mini-scan focusing on top 2 weakest pillars

assessment_examples:
  "0":
    - example: No well-architected assessment performed; pillar risks unknown.
    - example: Improvement actions absent.
  "1":
    - example: Single baseline assessment completed; recommendations stored in static document.
    - example: Action items lack owners or target dates.
  "2":
    - example: Second assessment done; some actions tracked manually; limited trend visibility.
    - example: Pillar scores recorded but not correlated with operational metrics.
  "3":
    - example: Remediation backlog maintained with owners & due dates; trend chart shows improving weak pillars.
    - example: Automated IaC & cost scans map to assessment findings.
  "4":
    - example: Rolling remediation burn-down & pillar score dashboard reviewed quarterly; sustainability considerations integrated.
    - example: Action closure rate improving; stale actions auto-flagged.
  "5":
    - example: Continuous evidence ingestion updates pillar scoring; predictive analytics highlight emerging risk.
    - example: Measurable correlation: improvements reduce incident frequency / cost anomalies.


---
