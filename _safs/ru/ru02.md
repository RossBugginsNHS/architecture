---
title: RU02
description: Reuse Potential
dimension: reuse
tags: [reuse]
nav_order: 2.62

requirement: |
  For new capabilities we **SHOULD** identify/recognise the potential reuse opportunities which may drive design decisions, benefits etc.

more_info: |
  Goal:
    Intentionally design new capabilities with future reuse potentialâ€”balancing
    generality and time-to-value.

  Reuse design heuristics:
    - Clear domain boundary & contract (avoid leaking internal concepts)
    - Configuration over code for variability
    - Idempotent, side-effect predictable operations
    - Extensible data schema (forward-compatible fields)
    - Observability exposing meaningful consumer metrics

  Identification process:
    1. Map capability to enterprise capability model
    2. Identify adjacent programmes with similar emerging needs
    3. Classify reuse likelihood (High / Medium / Low)
    4. Adjust abstraction level accordingly (YAGNI guardrails)

  Pitfalls:
    - Over-abstracting for hypothetical consumers
    - Embedding tenant-specific logic early
    - No discoverability (capability hidden in monolith internals)

examples: 
    - title: Capability Specification Document
      content: |
        Purpose, domain model, API surface, extension points.
    - title: Reuse Likelihood Matrix
      content: |
        Rows: Capability; Columns: Potential Consumer Groups; Score.
    - title: Abstraction Decision ADR
      content: |
        Justification for minimal generality now with revisit trigger.

technology:
    - title: API Catalogue / Portal
      content: |
        Makes candidate reusable interfaces discoverable.
    - title: Feature Flag System
      content: |
        Controls staged exposure for new consumers.
    - title: Schema Versioning Tool
      content: |
        Manages additive evolution.

further_reading:
    - title: Capability Mapping Guidance
      content: Aligning system functions to business capabilities.
      url: https://martinfowler.com/bliki/BusinessCapability.html
    - title: Designing for Reuse (Patterns)
      content: Principles reducing accidental coupling.
      url: https://www.thoughtworks.com/insights

assessment_guidance: |
  Assessment focus:
    Determine if new capability design balances immediate needs with pragmatic future reuse potential (avoiding speculative over-abstraction).

  Steps:
    1. Review capability specification: confirm boundary & anti-goals explicitly listed.
    2. Evaluate reuse likelihood classification & rationale vs market / internal demand signals.
    3. Inspect API / contract: ensure no tenant-specific logic or custom field proliferation.
    4. Check extension mechanism clarity (hooks, config) and absence of premature plugin complexity.
    5. Trace one potential external consumer pathway & identify missing abstraction layers (if any).

  Evidence:
    - Specification excerpt
    - Reuse likelihood matrix row
    - Contract snippet illustrating neutrality
    - Extension mechanism overview

  Red flags:
    - Generic interface with no current consumer and high maintenance cost
    - Hard-coded tenant configs disguised as general features
    - Lack of performance / scalability considerations for reuse scenario

  Maturity signals:
    - Reuse adoption metrics (calls, consumers) tracked & informing roadmap
    - Intentional de-scoping decisions captured with revisit triggers
    - Versioning strategy supporting additive evolution

  Quick improvements:
    - Add anti-goals section to spec
    - Introduce consumer feedback capture form
    - Implement contract linter for tenant leakage

assessment_examples:
  "0":
    - example: Capability built solely for immediate tenant with embedded tenant-specific logic.
    - example: No specification; reuse potential unconsidered.
  "1":
    - example: Specification drafted but lacks anti-goals and variability strategy; no consumer feedback.
    - example: Reuse likelihood guessed (High/Low) without evidence.
  "2":
    - example: Clear boundary and anti-goals documented; reuse likelihood classified with rationale.
    - example: Basic observability (request counts) in place; no adoption metrics yet.
  "3":
    - example: Versioning and extension hooks defined; early external (or other team) consumer engaged in sandbox.
    - example: Adoption and error metrics feed prioritisation; abstraction level adjusted once to remove premature generality.
  "4":
    - example: Reuse metrics (unique consumers, latency per operation) tracked and drive roadmap increments.
    - example: Intentional de-scoping decisions recorded with revisit triggers executed on schedule.
  "5":
    - example: Automated contract lint prevents tenant leakage; consumer feedback loop integrated into planning cadence.
    - example: Reuse ROI dashboard correlates adoption to reduced duplicate build cost.

---
