---
title: DM08
dimension: decisions
tags: [decision making,drivers,dependencies,context,assumptions,forces,impact-analysis,traceability,options]
nav_order: 2.28

description: Structured Decision Making

requirement: |
  All decision-making **SHOULD** be structured. E.g. identify key strategic drivers, user need assess options against drivers, present rationale, clarity on trade-offs, dependencies, risks and issues understood.

more_info: |
  Objective:
    Ensure material decisions follow a structured, transparent evaluation so
    selection is defensible and aligned to strategic drivers & user needs.

  Structured process:
    1. Frame decision (problem statement, scope boundaries)
    2. Identify drivers (strategic, user, compliance, constraints)
    3. Generate options (include baseline / do-nothing)
    4. Define evaluation criteria (weighted if appropriate)
    5. Assess options (qualitative + quantitative evidence)
    6. Recommend & record trade-offs (explicitly list discarded options)
    7. Capture follow-up actions and review triggers

  Evaluation criteria examples:
    - Alignment to principles / target architecture
    - Time-to-value & delivery risk
    - Operability / support model maturity
    - Cost profile (initial + run + exit)
    - Security / compliance posture
    - Reuse enablement & ecosystem fit

  Pitfalls:
    - Criteria retrofitted to justify pre-selected option
    - Ignoring intangible or ecosystem impacts (capability fragmentation)
    - Over-weighting short-term cost vs strategic resilience

examples: 
    - title: Decision Evaluation Table
      content: |
        Rows: Options; Columns: Criteria (score + rationale) with weighted total.
    - title: Trade-off Summary Block
      content: |
        Section explicitly listing benefits forfeited & risks accepted.
    - title: Revisit Trigger List
      content: |
        Conditions (usage threshold, vendor price change) prompting re-evaluation.

technology:
    - title: Markdown Decision Template
      content: |
        Pre-defined sections for drivers, criteria, options, trade-offs.
    - title: Spreadsheet / Scoring Tool
      content: |
        Weighted evaluation and sensitivity analysis.
    - title: Diagramming Tool
      content: |
        Visualises option architecture deltas.

further_reading:
    - title: Architecture Evaluation Criteria (ATAM inspiration)
      content: Scenario-based evaluation concepts.
      url: https://resources.sei.cmu.edu/library/asset-view.cfm?assetid=512085
    - title: Trade-off Analysis Patterns
      content: Approaches to documenting conscious trade-offs.
      url: https://martinfowler.com/

assessment_guidance: |
  Assessment focus:
    Ensure decision process is structured, unbiased and transparent from framing to trade-off capture & revisit triggers.

  Steps:
    1. Review a recent evaluation pack: confirm explicit problem statement, drivers, weighted criteria, options (incl. baseline) and scoring rationale.
    2. Validate scoring consistency: pick two criteria & ensure scoring rationale differentiates options (not copy-paste).
    3. Inspect trade-off section: risks / benefits of rejected options clearly noted?
    4. Check revisit triggers presence (what condition demands re-evaluation?).
    5. Confirm at least one historical decision was revisited based on trigger event or changed assumptions.

  Evidence:
    - Evaluation matrix screenshot
    - Scoring rationale excerpts
    - Trade-off summary block
    - Revisit log entry

  Red flags:
    - Criteria invented post selection (“score laundering”)
    - Identical scores across non-trivial options
    - Missing baseline / do-nothing option
    - No documented revisit triggers

  Maturity signals:
    - Sensitivity analysis provided (weight perturbation impact)
    - Automated template generating evaluation tables
    - Historical decisions annotated with actual outcome vs expected

  Quick improvements:
    - Introduce annex explaining scoring scale per criterion
    - Add section “Assumptions & Validation Plan” to each decision pack
    - Track revisit triggers in a lightweight register

assessment_examples:
  '0':
    - example: Decisions made without structured options or criteria; chosen path undocumented.
    - example: No baseline / do-nothing consideration.
  '1':
    - example: Basic template used but criteria vague; scoring subjective & unrecorded.
    - example: Alternatives listed without rationale or trade-offs.
  '2':
    - example: Criteria defined; options scored qualitatively; weighting absent or informal.
    - example: Revisit triggers rarely stated; discarded options rationale thin.
  '3':
    - example: Weighted evaluation matrix with qualitative + quantitative evidence; baseline included.
    - example: Trade-offs & discarded option impacts explicitly captured; revisit triggers defined.
  '4':
    - example: Sensitivity analysis performed (weight perturbation) influencing confidence statement.
    - example: Validation / assumption tracking plan created with checkpoint dates.
  '5':
    - example: Structured decision tooling auto-generates evaluation & sensitivity outputs.
    - example: Historical decisions annotated with outcome vs expected metrics; triggers enacted leading to adaptive revisions.
    - example: Portfolio of decisions searchable by driver / criterion for cross-learning.






---
  