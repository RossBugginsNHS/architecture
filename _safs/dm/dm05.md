---
title: DM05
dimension: decisions
tags:
- design authorities
- design
- lifecycle
- decisions
- governance
- review-board
- accountability
- standards
- assurance
- escalation
nav_order: 2.25
deprecated: false
description: Design Authorities
requirement: |-
  The solution design and architecture decisions **SHOULD** be managed through a tiered governance process, ultimately leading, if appropriate, to TRG.

  The decision process:

  Decisions **SHOULD** be made at the lowest possible level, with the appropriate stakeholders involved. Considerations include:

  - All decisions **SHOULD** follow local governance processes, such as a programme design authority.
  - If the solution requires assessment against Principles, Policies, Patterns and Standards then presentation to PDG or TRG would be appropriate.
  - At the appropriate stages of the lifecycle, all large items **MUST** go to TRG.
  - The relevant Lead Architects and Subject Matter Experts [(SMEs)](https://architecture.digital.nhs.uk/trg/role-of-ccc) **SHOULD** be engaged and supportive.
more_info: |
  Aim:
    Provide proportionate governance through local design authorities and TRG
    (where applicable) to ensure coherence, reuse, compliance and strategic fit.

  Authority model:
    - Local Programme Design Authority: day-to-day contextual decisions
    - Enterprise / TRG: cross-cutting impact, strategic alignment, exceptions
    - Specialist forums: security, data, clinical safety as required

  Submission expectations:
    - Problem & context summary (why now, drivers)
    - Options & trade-off analysis (with recommendation)
    - Impact (cost, risk, reuse, decommissioned assets)
    - Compliance mapping (principles, standards, red lines)
    - Decision required & time sensitivity

  Flow:
    1. Pre-engagement (scoping) with lead architect
    2. Pack / ADR preparation (concise delta-oriented)
    3. Circulate pre-read (time-boxed async questions)
    4. Authority session: clarify, challenge, record decision
    5. Update artefacts & publish outcomes (including conditions)

  Pitfalls:
    - Overloaded central forum deciding operational minutiae
    - Slide-decks as source-of-truth (lose change history)
    - Decisions conditional but follow-up tracking absent
examples:
- title: Decision Pack Summary Page
  content: 'Single page: context, options, recommendation, principle alignment, impacts.

    '
- title: Authority Decision Log Extract
  content: 'Meeting date, decision ID, outcome (approved w/ conditions), actions &
    owners.

    '
- title: Condition Closure Evidence
  content: 'Ticket referencing decision ID with artefact update PR.

    '
technology:
- title: Decision Register (Markdown / DB)
  content: 'Canonical list of authority outcomes & status.

    '
- title: Virtual Board / Workflow Tool
  content: 'Tracks submissions through stages (draft -> scheduled -> decided -> closed).

    '
- title: Automated Template Generator
  content: 'Produces pack skeleton referencing principle catalogue.

    '
further_reading:
- title: Lightweight Architecture Decision Making
  content: Guidance on optimising governance flow.
  url: https://martinfowler.com/bliki/ArchitectureDecision.html
- title: NHS Architecture Principles
  content: Reference for principle alignment sections.
  url: https://architecture.digital.nhs.uk/
assessment_guidance: |
  Assessment focus:
    Evaluate whether design authority processes deliver timely, high-quality, traceable decisions with managed conditions.

  Steps:
    1. Obtain authority decision log: sample 5 entries (mix of ages) for completeness (context, outcome, conditions, links).
    2. Identify conditional approvals: verify actions tracked to closure; any stale conditions?
    3. Measure lead time: submission date → decision date across last 10 items; identify bottlenecks.
    4. Inspect one rejected or deferred submission: check clarity of remediation guidance.
    5. Confirm decisions propagate to artefacts (diagrams / ADRs updated within agreed window).

  Evidence:
    - Decision log excerpt with timestamps
    - Condition tracking board / ticket references
    - Lead time distribution (chart or table)
    - Updated artefact PR referencing decision ID

  Red flags:
    - Growing backlog of pending submissions
    - Repeated emergency decisions outside normal path
    - Conditions closed with no objective verification evidence

  Maturity signals:
    - Automated status dashboard (queue length, SLA adherence)
    - <10% decisions require escalation for quorum issues
    - Lightweight template differentials (delta-based updates) reduce cycle time

  Quick improvements:
    - Introduce submission quality checklist
    - Add service-level targets (e.g. 80% decisions within 10 working days)
    - Automate condition reminder notifications
assessment_examples:
  '0':
  - example: No design authority involvement; major decisions made informally.
  - example: Outcomes undocumented or scattered across emails / slides.
  '1':
  - example: Ad‑hoc escalation to central forum; decisions recorded inconsistently;
      conditional approvals rarely tracked.
  - example: Long lead times with no measurement.
  '2':
  - example: Authority log exists capturing outcome & date; conditions tracked manually
      with gaps.
  - example: Average lead time high with occasional bottleneck analysis.
  '3':
  - example: Standardised submission template; log includes conditions & closure evidence
      links.
  - example: Lead time metrics produced; conditional approvals monitored (<20% overdue).
  '4':
  - example: Automated dashboard (queue length, SLA adherence, condition ageing) reviewed
      regularly.
  - example: "<10% conditions overdue; majority of submissions delta-based reducing
      cycle time."
  '5':
  - example: Continuous optimisation of authority scope (right-sizing); median decision
      lead time improving quarter-on-quarter.
  - example: Zero stale conditions >1 review cycle; governance analytics drive proactive
      load balancing.
  - example: Decision + condition data feeds strategic planning & risk mitigation.
---