---
title: NF03
description: Performance
dimension: non functional
tags: [performance,volume]
nav_order: 2.54

requirement: |
  An overall volume and performance model **SHOULD** exist and includes business-realistic exceptional scenarios.

more_info: |
  Intent:
    Understand and validate system performance characteristics under expected
    and exceptional business scenarios to prevent capacity shocks.

  Model components:
    - Workload taxonomy (transaction types, payload size distribution)
    - Growth projections (seasonal, campaign-driven, organic)
    - Bottleneck analysis (CPU, IO, network, external APIs)
    - Exceptional scenarios (outbreak spike, regulatory deadline surge)

  Validation cycle:
    1. Baseline benchmark & profiling
    2. Stress / soak / spike tests
    3. Performance budget enforcement (pipeline checks)
    4. Regression tracking (capacity vs release deltas)

  Pitfalls:
    - Chasing synthetic benchmarks misaligned to real usage mix
    - One-off load test before go-live only
    - Excessive headroom (wasted cost) due to lack of auto-scaling confidence

examples: 
    - title: Performance Profile Report
      content: |
        Flame graph identifying cryptographic routine hotspot.
    - title: Spike Test Result Graph
      content: |
        TPS vs latency curve with saturation knee highlighted.
    - title: Capacity Planning Sheet
      content: |
        Growth assumptions and scaling event triggers.

technology:
    - title: Load Testing (k6 / Locust)
      content: |
        Executes scenario-based performance tests.
    - title: Profiler (py-spy / JFR)
      content: |
        Identifies code-level hotspots.
    - title: Auto-scaling Policies
      content: |
        Adjusts capacity responding to metrics.

further_reading:
    - title: SRE Workbook (Capacity Planning)
      content: Techniques for predictive planning.
      url: https://sre.google/books/

assessment_guidance: |
  Assessment focus:
    Determine if performance model is realistic, validated continuously and driving architecture adaptation.

  Steps:
    1. Review workload taxonomy: confirm representative mix & growth assumptions with data references.
    2. Inspect latest performance test report: highlight capacity threshold (saturation knee) & compare to projected demand horizon.
    3. Examine regression tracking: show performance delta across last 3 releases.
    4. Validate exceptional scenario modeling (e.g. 5x spike) and mitigation plan (auto-scale / backpressure).
    5. Look at optimisation ADRs referencing profile evidence (flame graphs or metrics) not hunches.

  Evidence:
    - Taxonomy & projection table
    - Load test graph with saturation mark
    - Release performance delta summary
    - Exceptional scenario response plan

  Red flags:
    - Synthetic tests ignoring expensive real endpoints/payloads
    - Manual scaling only (no automation) for predictable bursts
    - Repeated regression with no guardrail failing build

  Maturity signals:
    - Performance budgets enforced pre-merge
    - Auto-scaling events correlated with SLO adherence dashboards
    - Predictive modelling updated quarterly with actual telemetry

  Quick improvements:
    - Introduce labelled performance test scenarios (baseline, spike, soak)
    - Add performance regression gate (threshold diff)
    - Establish optimisation decision template referencing metrics

---
