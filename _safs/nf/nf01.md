---
title: NF01
description: Observability
dimension: non functional
tags: [observability,health,events]
nav_order: 2.51

requirement: |
    Solutions **MUST** incorporate workload observability, understand service health, and respond to events  

more_info: |
  Intent:
    Provide deep visibility into system behaviour so teams can detect anomalies,
    triage rapidly, and drive continuous improvement.

  Observability pillars:
    - Metrics: RED/USE (Rate, Errors, Duration / Utilisation, Saturation, Errors)
    - Logs: structured, contextual, correlation IDs
    - Traces: distributed spans across service boundaries
    - Events: domain and audit events for business insight

  Implementation essentials:
    - Standard logging schema (timestamp, service, requestId, userRef)
    - Propagate correlation context (trace/span IDs) end-to-end
    - SLO definition: target, measurement window, error budget policy
    - Alert design: symptom not cause, actionable, owned

  Maturity indicators:
    - < 5% noisy / unactionable alerts
    - MTTR trending down quarter-on-quarter
    - Error budget burn informs release pace
    - Trace sampling adaptive (high fidelity for errors)

  Pitfalls:
    - Oversampling everything -> cost / signal dilution
    - Chatty log statements lacking structure
    - Alerts on raw infrastructure metrics without user impact context

examples: 
    - title: Service SLO Definition
      content: |
        Availability 99.9% monthly; Latency p95 < 250ms; Error budget 43 mins downtime.
    - title: Trace Waterfall Snapshot
      content: |
        Span breakdown showing cache miss impact on overall latency.
    - title: Alert Runbook Entry
      content: |
        Trigger condition, hypothesis checklist, diagnostic queries.

technology:
    - title: Metrics Stack (Prometheus / OpenTelemetry Collector)
      content: |
        Scrapes, normalises and forwards metrics.
    - title: Tracing (Tempo / Jaeger)
      content: |
        Distributed tracing store & UI.
    - title: Log Aggregation (Loki / Elasticsearch)
      content: |
        Centralised structured logs & query interface.
    - title: Alert Manager
      content: |
        Routing & deduplication with escalation policies.

further_reading:
    - title: Google SRE Handbook (SLOs)
      content: Defining & operating with error budgets.
      url: https://sre.google/books/
    - title: OpenTelemetry Docs
      content: Unified observability instrumentation standard.
      url: https://opentelemetry.io/

assessment_guidance: |
  Assessment focus:
    Determine if observability delivers actionable insight (SLO-driven) vs raw data exhaust.

  Steps:
    1. Review SLO definitions: verify target, window, error budget & policy for at least latency & availability.
    2. Inspect alert catalogue: sample 5 alerts—each must have runbook link & explicit ownership.
    3. Trace one incident: show detection signal → alert → response timeline → learning feeding improvement (SLO / code / infra).
    4. Examine tracing coverage: select a request; ensure spans cover critical downstream services (no dark areas).
    5. Validate log schema consistency across two services (fields & correlation IDs present).

  Evidence:
    - SLO table excerpt
    - Alert definitions & runbook links
    - Incident postmortem referencing observability artifacts
    - Trace waterfall screenshot

  Red flags:
    - Alert noise ratio high (muted alerts, ignored channels)
    - Missing correlation IDs preventing cross-system diagnostics
    - SLOs declared but not measured in dashboards

  Maturity signals:
    - Error budget burn influencing release velocity decisions
    - Adaptive sampling reducing cost without losing insight
    - Observability as code (config managed & versioned)

  Quick improvements:
    - Add runbook_url required field to alert definitions
    - Implement golden signals dashboard template
    - Automate tracing instrumentation coverage report

assessment_examples:
  "0":
    - example: No SLOs, only ad-hoc dashboards of host metrics; alerts rarely actionable or routinely ignored.
    - example: Logs unstructured and lack correlation IDs making multi-service tracing impossible.
  "1":
    - example: Basic uptime & error rate metrics captured but no targets or error budget concept defined.
    - example: Some services emit trace spans but coverage is patchy with large gaps across critical calls.
  "2":
    - example: Core golden signals instrumented and a handful of SLOs drafted; alert ownership inconsistently assigned.
    - example: Runbooks exist for high severity alerts, but MTTR data not yet feeding improvement work.
  "3":
    - example: SLOs (latency & availability) defined with error budgets influencing release freeze decisions.
    - example: Distributed tracing consistently propagated; sampling tuned manually for high-value transactions.
  "4":
    - example: Alert catalogue pruned with <5% noise; adaptive sampling and exemplars for high latency investigations.
    - example: Post-incident reviews reference observability data and create follow-up backlog items tracked to closure.
  "5":
    - example: Observability as code (dashboards, alerts, SLOs) versioned; automated coverage reports gating merges.
    - example: Error budget burn and user experience telemetry integrated into automated release & rollback decisions.




---
