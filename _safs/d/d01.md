---
title: D01
description: Documentation Stores
dimension: documentation
tags: [documentation,knowledge]
nav_order: 2.71

requirement: |
  All architecture documentation **SHOULD** be maintained (with supporting processes and change control) within the appropriate NHS England knowledge store(s) e.g. Aalto, SharePoint, Confluence.

more_info: |
  Purpose:
    Architecture information must be easy to find, trusted, and governed. A single
    authoritative location (with clearly sign‑posted exceptions) reduces time lost
    hunting for diagrams, decisions and models and lowers the risk of teams
    operating from stale copies.

  Scope of "architecture documentation":
    - Direction (vision, principles, roadmap)
    - Designs (context, logical / physical views, data, integration)
    - Decisions (ADR / KAD / debt register)
    - Non‑functional & risk material (NFRs, threat / resilience aspects)

  Store selection guidelines:
    - One canonical store per document type (e.g. ADR index in Git, models in Archi repo)
    - Discoverable via an index / landing page
    - Supports version history & access control (read for all, write restricted)
    - Supports export / retention policies

  Governance & lifecycle:
    - Every artefact has an owner (role not person) and a review cadence
    - Arch debt & retired components are marked clearly (do not delete context)
    - Change control references issue / PR / ticket IDs
    - Links to related artefacts maintained (e.g. decision -> impacted diagram)

  Evidence you can supply:
    - Repository / space index page enumerating doc types and locations
    - ADR index with stable IDs and dates
    - Access model (who can contribute / approve)

  Common pitfalls:
    - Splitting canonical docs across Confluence + SharePoint + local folders
    - Hosting diagrams only inside presentation decks (losing diff history)
    - Personal OneDrive / desktop copies treated as source

  Quality checklist heuristic:
    - Findability: < 2 clicks from landing page
    - Freshness: Last review within agreed cadence (e.g. 90 days)
    - Traceability: Each design links to at least one decision & principle

examples: 
    - title: Canonical Architecture Index Page
      content: |
        A README (or Confluence landing page) listing each artefact type, the
        canonical location, owner role, and review frequency.
    - title: Decision Log Extract
      content: |
        Table of ADR IDs with status (accepted / superseded) and links to the
        corresponding solution diagrams illustrating the impact.
    - title: Repository Structure Screenshot
      content: |
        Example Git tree showing /docs (markdown), /decisions (ADRs), /models
        (Archi) and /diagrams (plantuml) enforcing predictable layout.

technology:
    - title: Confluence / SharePoint
      content: |
        Corporate knowledge stores for wider audience consumption & governance packs.
    - title: Git / GitHub Enterprise
      content: |
        Docs‑as‑Code (markdown, ADRs, version control, code review workflow).
    - title: Aalto (or Architecture Registry)
      content: |
        Catalogue / inventory of solutions & higher level artefacts.
    - title: Backstage / Developer Portal (optional)
      content: |
        Surfacing architecture metadata and linking operational docs.
    - title: PlantUML / Structurizr / Mermaid
      content: |
        Diagram as code enabling diffable, reviewable design artefacts.

further_reading:
    - title: ADR (Architecture Decision Record) Guidance
      content: Lightweight pattern for recording architectural decisions.
      url: https://adr.github.io/
    - title: GDS Service Manual - Documentation
      content: Government guidance on making technical documentation discoverable.
      url: https://www.gov.uk/service-manual/technology
    - title: Docs as Code (ThoughtWorks Radar)
      content: Rationale for versioning documentation alongside source.
      url: https://www.thoughtworks.com/radar/techniques/docs-as-code
    - title: NHS Service Standard
      content: Expectations influencing documentation transparency & governance.
      url: https://service-manual.nhs.uk/service-standard

assessment_guidance: |
  How to assess (Solution Architect focus):
    1. Locate the single authoritative landing page / index for architecture docs.
       Confirm it enumerates ALL required artefact types (vision, decisions, models etc.) or states why absent.
    2. Pick a recent significant change (e.g. new component). Trace: index → design doc → ADR → relevant diagram commit. If any hop fails, note gap.
    3. Sample 3 ADRs. Check rationale clarity, linkage to impacted artefacts and status accuracy.
    4. Inspect freshness: at least one artefact each from strategy, design, decision & risk should show a last_verified date within agreed cadence.
    5. Verify permissions: broad read access, controlled write (no personal silos). Spot-check for shadow copies (old Confluence space / slide deck).

  Evidence to gather:
    - Screenshot / snippet of documentation index.
    - Traceability chain example (list the linked URIs).
    - ADR sample IDs with last modified + status.
    - Freshness metadata table (artefact, last_verified, next_review_due).
    - Access / repo permission matrix.

  Questions to ask:
    - What prevents divergence / local copies forming today?
    - Who owns keeping the index current? Is that a role, not a person?
    - How is retirement / archival of obsolete docs handled?
    - Are diagrams stored as code or binary-only? (Diff & review implications.)

  Common anti‑patterns:
    - Multiple wikis claiming to be canonical.
    - Diagrams only embedded in presentations (no source of truth).
    - ADRs missing alternatives / consequences.
    - Decision rationale living solely in meeting minutes.

  Maturity indicators:
    - All artefacts link back to index (no orphan markdown).
    - Automated check (script / pipeline) validating presence & metadata.
    - >90% artefacts within freshness SLA.
    - Clear ownership roster (role-based) for each artefact class.

  Quick win recommendations (if weak):
    - Create a docs/README.md index with a simple table (Type | Location | Owner | Review cadence).
    - Introduce last_verified front matter & a weekly script to flag overdue.
    - Convert static diagrams to diagram-as-code for diffable change history.
    - Add an ADR template & pre-commit hook to enforce required sections.

assessment_examples:
  '0': |
    Characteristics:
      - Cannot produce a single authoritative index / landing page.
      - Multiple conflicting stores (e.g. scattered Confluence spaces + ad‑hoc SharePoint folders + local PPT decks).
      - Diagrams only exist as images inside slide decks; no diagram source files.
      - No ADR / decision log (decisions in email or meeting minutes only).
      - No stated owners or review cadence; freshness unknowable.
      - Personal OneDrive used as de‑facto source.
    Indicative “evidence” when challenged: verbal explanation or outdated slide pack (>12 months old) with no link to current design.
  '1': |
    Characteristics:
      - Partial index exists but missing key artefact categories (e.g. no risk / NFR docs, no decision list).
      - Some documents versioned; others in static binary form (PPT / Visio) without history.
      - ADR log started but contains <5 entries and lacks alternatives / status fields.
      - Ownership unclear for majority (>70%) of artefacts; few last_verified fields present.
      - Duplicated copies of the same diagram across stores; unsure which is current.
    Evidence quality: patchy, inconsistent formatting; difficult to trace a change end‑to‑end.
  '2': |
    Characteristics:
      - Single index page present covering core areas (design, decisions, some diagrams) but omits strategy or non‑functional artefacts.
      - ADR log captures a fair subset of significant decisions but missing superseded chain or rationale depth.
      - Mixed diagram practices: ~50% diagram-as-code, remainder static images.
      - Freshness metadata applied to a minority (<50%) of artefacts; several obviously stale (> cadence threshold).
      - Owners assigned for high-impact documents only.
    Evidence shows intent and partial process, but gaps hinder full traceability.
  '3': |
    Characteristics:
      - Index enumerates most artefact types with live links (small omissions, e.g. resilience view or debt register linkage missing).
      - ADR log active (decisions for last quarter captured) but a few significant design pivots undocumented.
      - Majority diagrams managed as code; a handful of legacy binaries awaiting migration.
      - Freshness coverage ~70% within SLA; overdue items flagged manually (no automation).
      - Ownership roles defined for most artefact classes; escalation path informal.
    Traceability works for common flows but occasional dead links or undocumented rationale appear.
  '4': |
    Characteristics:
      - Comprehensive index with all standard artefact categories plus clear “intentionally absent” notes where applicable.
      - ADR log complete with status transitions & supersession chains; rare backlog of pending ADRs.
      - >85–90% artefacts within freshness SLA; exceptions have review dates & remediation tasks.
      - Diagrams predominantly (≥90%) as code with consistent naming & version history.
      - Permission model documented (broad read, controlled write); auditing of changes via PRs / issues.
      - Emerging automation (link checker / freshness script) present but not fully enforced in CI.
    Only minor duplication or a small number of legacy artefacts awaiting archival.
  '5': |
    Characteristics:
      - Single authoritative index auto-generated from structured metadata (no manual drift) covering every artefact class.
      - 100% of in-scope artefacts have owner role, last_verified, next_review_due; dashboard surfaces SLA adherence in real time.
      - Full decision traceability: design doc ↔ ADR ↔ diagram commit ↔ principle / objective links validated by pipeline.
      - All diagrams as code with lint / consistency checks; no orphan or duplicate artefacts.
      - Automated CI gates: missing metadata or broken links fail build; freshness reminders created as issues.
      - Access model least‑privilege with broad read; periodic permission audit evidence retained.
      - Continuous improvement loop: metrics (findability, freshness %, decision coverage) trending positively quarter-on-quarter.
    Evidence package can be produced rapidly (scripted export) demonstrating compliance end‑to‑end.







---
