---
title: D01
description: Documentation Stores
dimension: documentation
tags: [documentation,knowledge]
nav_order: 2.71

requirement: |
  All architecture documentation **SHOULD** be maintained (with supporting processes and change control) within the appropriate NHS England knowledge store(s) e.g. Aalto, SharePoint, Confluence.

more_info: |
  Purpose:
    Architecture information must be easy to find, trusted, and governed. A single
    authoritative location (with clearly sign‑posted exceptions) reduces time lost
    hunting for diagrams, decisions and models and lowers the risk of teams
    operating from stale copies.

  Scope of "architecture documentation":
    - Direction (vision, principles, roadmap)
    - Designs (context, logical / physical views, data, integration)
    - Decisions (ADR / KAD / debt register)
    - Non‑functional & risk material (NFRs, threat / resilience aspects)

  Store selection guidelines:
    - One canonical store per document type (e.g. ADR index in Git, models in Archi repo)
    - Discoverable via an index / landing page
    - Supports version history & access control (read for all, write restricted)
    - Supports export / retention policies

  Governance & lifecycle:
    - Every artefact has an owner (role not person) and a review cadence
    - Arch debt & retired components are marked clearly (do not delete context)
    - Change control references issue / PR / ticket IDs
    - Links to related artefacts maintained (e.g. decision -> impacted diagram)

  Evidence you can supply:
    - Repository / space index page enumerating doc types and locations
    - ADR index with stable IDs and dates
    - Access model (who can contribute / approve)

  Common pitfalls:
    - Splitting canonical docs across Confluence + SharePoint + local folders
    - Hosting diagrams only inside presentation decks (losing diff history)
    - Personal OneDrive / desktop copies treated as source

  Quality checklist heuristic:
    - Findability: < 2 clicks from landing page
    - Freshness: Last review within agreed cadence (e.g. 90 days)
    - Traceability: Each design links to at least one decision & principle

examples:
  - title: Canonical Architecture Index Page
    content: |
      A README (or Confluence landing page) listing each artefact type, the
      canonical location, owner role, and review frequency.
  - title: Decision Log Extract
    content: |
      Table of ADR IDs with status (accepted / superseded) and links to the
      corresponding solution diagrams illustrating the impact.
  - title: Repository Structure Screenshot
    content: |
      Example Git tree showing /docs (markdown), /decisions (ADRs), /models
      (Archi) and /diagrams (plantuml) enforcing predictable layout.

technology:
  - title: Confluence / SharePoint
    content: |
      Corporate knowledge stores for wider audience consumption & governance packs.
  - title: Git / GitHub Enterprise
    content: |
      Docs‑as‑Code (markdown, ADRs, version control, code review workflow).
  - title: Aalto (or Architecture Registry)
    content: |
      Catalogue / inventory of solutions & higher level artefacts.
  - title: Backstage / Developer Portal (optional)
    content: |
      Surfacing architecture metadata and linking operational docs.
  - title: PlantUML / Structurizr / Mermaid
    content: |
      Diagram as code enabling diffable, reviewable design artefacts.

further_reading:
  - title: ADR (Architecture Decision Record) Guidance
    content: Lightweight pattern for recording architectural decisions.
    url: https://adr.github.io/
  - title: GDS Service Manual - Documentation
    content: Government guidance on making technical documentation discoverable.
    url: https://www.gov.uk/service-manual/technology
  - title: Docs as Code (ThoughtWorks Radar)
    content: Rationale for versioning documentation alongside source.
    url: https://www.thoughtworks.com/radar/techniques/docs-as-code
  - title: NHS Service Standard
    content: Expectations influencing documentation transparency & governance.
    url: https://service-manual.nhs.uk/service-standard

assessment_guidance: |
  How to assess (Solution Architect focus):
    1. Locate the single authoritative landing page / index for architecture docs.
       Confirm it enumerates ALL required artefact types (vision, decisions, models etc.) or states why absent.
    2. Pick a recent significant change (e.g. new component). Trace: index → design doc → ADR → relevant diagram commit. If any hop fails, note gap.
    3. Sample 3 ADRs. Check rationale clarity, linkage to impacted artefacts and status accuracy.
    4. Inspect freshness: at least one artefact each from strategy, design, decision & risk should show a last_verified date within agreed cadence.
    5. Verify permissions: broad read access, controlled write (no personal silos). Spot-check for shadow copies (old Confluence space / slide deck).

assessment_examples:
  '0':
    - example: Cannot produce a single authoritative index / landing page.
    - example: Multiple conflicting stores (scattered Confluence spaces, ad‑hoc SharePoint folders, local PPT decks).
    - example: Diagrams only exist as static images inside slide decks; no diagram source files (PlantUML / Structurizr) available.
    - example: No ADR / decision log (decisions captured only in email or meeting minutes).
    - example: No stated owners or review cadence; freshness of artefacts unknowable.
    - example: Personal storage (OneDrive/Desktop) acting as de‑facto source of truth.
    - example: Attempts to trace a recent change fail (cannot link design → decision → diagram).
    - example: Evidence is verbal or an outdated (>12 months) slide pack with obsolete component names.
  '1':
    - example: Partial index exists but omits key categories (e.g. risk register, NFR set, roadmap, data models).
    - example: Some documents versioned in Git; others remain binary (Visio / PPT) with no meaningful history.
    - example: ADR log started (<5 entries) lacking alternatives, consequences, or status field (accepted / superseded).
    - example: Ownership unclear for most artefacts; a few last_verified fields appear sporadically.
    - example: Duplicate diagrams across locations; “latest” cannot be determined confidently.
    - example: Freshness claims based on last modified timestamps rather than explicit verification.
    - example: Traceability is patchy; reviewers spend time reconciling duplicates.
  '2':
    - example: Single index covers core design views and a subset of decisions; strategy / risk / NFR artefacts still missing or linked to outdated spaces.
    - example: ADR log captures notable decisions for last few months but supersession chains and rationale depth inconsistent.
    - example: Mixed diagram practice (~50% diagram‑as‑code, remainder static images without lineage).
    - example: Freshness metadata applied to a minority (<50%) of artefacts; several clearly beyond cadence threshold.
    - example: Owners assigned only for critical documents; others implicitly owned.
    - example: Manual effort required to establish whether an artefact is current.
    - example: Intent visible yet gaps hinder reliable reuse and onboarding.
  '3':
    - example: Index enumerates most artefact types (minor omissions e.g. resilience view or debt register linkage) with working hyperlinks.
    - example: ADR log active for recent quarter; a few significant pivots not yet documented (tracked in backlog).
    - example: Majority diagrams now managed as code; handful of legacy binaries awaiting migration plan.
    - example: Freshness coverage ~70% within SLA; overdue items manually flagged (spreadsheet or ad‑hoc script; not CI‑enforced).
    - example: Clear owner roles for most artefact classes; escalation handling informal.
    - example: Traceability chain (index → doc → ADR → diagram) works for common flows; occasional dead links occur.
    - example: Foundation established; automation & coverage must improve for higher scores.
  '4':
    - example: Comprehensive index includes all standard categories plus intentionally absent notes (with rationale) to prevent ambiguity.
    - example: ADR log complete with status transitions and supersession chains; pending ADR backlog short & tracked.
    - example: from >85–90% artefacts within freshness SLA; remaining items have explicit remediation tickets or scheduled reviews.
    - example: Diagrams predominantly (≥90%) as code with naming conventions & lint / review checks applied.
    - example: Permission & contribution model documented; PR / issue references provide change audit trail.
    - example: Emerging automation (link checker, freshness script) runs on schedule; warnings surfaced in dashboard (not yet gating merges).
    - example: Low duplication; legacy artefacts archived with forward/back links.
    - example: Only incremental automation & enforcement gaps separate from score 5.
  '5':
    - example: Single authoritative index auto‑generated from structured metadata (no manual drift) covering all artefact classes and cross‑links.
    - example: 100% artefacts have owner role, last_verified, next_review_due; real‑time dashboard shows findability, freshness & decision coverage metrics.
    - example: Full decision traceability (design doc ↔ ADR ↔ diagram commit ↔ principle/objective) validated by CI.
    - example: All diagrams as code; automated style, link & orphan checks; zero duplicate/orphaned artefacts in latest run.
    - example: CI gates fail on missing metadata, broken links, or overdue freshness; issues auto‑raised tagging the responsible owner role.
    - example: Access model least‑privilege with periodic permission audit evidence linked from index.
    - example: "Continuous improvement loop: key metrics trending positively quarter‑on‑quarter; improvement actions tracked to closure."
    - example: Evidence export script produces complete compliance pack (index, metrics, traceability samples) in minutes.
    - example: Documentation operates as a governed product with observable quality signals.

---

