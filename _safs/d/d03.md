---
title: D03
description: Documentation Freshness
dimension: documentation
tags: [documentation,consistency,style-guide,templates,review-process,governance,quality,automation,metadata]
nav_order: 2.73

requirement: |
  The architecture documentation **SHOULD** be appropriate in scope and quality for the solution covering (but not exclusively): Architecture Vision Architecture Roadmap Layer Diagrams Capability Model and Solution Mapping Non functional requirements Conceptual Architecture Logical Architecture Physical (including network, infrastructure etc.) Solution Architecture Overview (SDO) Key Architecture Decisions (KADs) Data Models Data Flows API Specifications Volume and Performance Models Architecture Decision Records Assumption, Risks, Issues and Dependencies Cyber Assessment Framework compliance

more_info: |
  Goal:
    Ensure consumers trust that architecture artefacts reflect current intent,
    decisions and constraints—minimising rework caused by stale documentation.

  Freshness model:
    - Assign each artefact a review SLA (e.g. quarterly for roadmap, monthly for risk register)
    - Track last_verified date separate from last_modified (explicit validation)
    - Auto-flag overdue items (dashboard / badge) to trigger review

  Recommended metadata fields:
    - owner_role: e.g. "Solution Architect"
    - last_verified: ISO timestamp of confirmation pass
    - next_review_due: derived from cadence
    - status: current / superseded / archived

  Process outline:
    1. Inventory baseline (list all required artefacts—see D06 catalogue)
    2. Define cadence per artefact type based on volatility & risk
    3. Automate reminders (scheduler / GitHub Actions / calendar)
    4. Record verification outcome (links to PR / ticket evidence)
    5. Surface metrics (freshness % by portfolio / programme)

  Metrics / KPIs:
    - Fresh artefact coverage = (in-date artefacts / total) * 100
    - Average days overdue (for out-of-date set)
    - Mean verification latency (difference between due and actual verify date)

  Evidence examples:
    - Dashboard screenshot showing 95% freshness with list of overdue items
    - PR referencing verification updates to multiple ADR statuses
    - Automated job log updating last_verified fields

  Pitfalls:
    - Treating any file save as "review" (needs explicit verification)
    - Ignoring superseded artefacts instead of marking archived with rationale
    - Over-frequent cadences generating alert fatigue

examples: 
    - title: Freshness Dashboard
      content: |
        A panel listing architecture artefacts with colour-coded SLA status
        (green in-date, amber due soon, red overdue) and aggregate percentage.
    - title: Verification Pull Request
      content: |
        PR updating last_verified and next_review_due across multiple markdown
        files with a link to meeting minutes confirming no content change.
    - title: Out-of-Date Alert Ticket
      content: |
        Automatically raised issue referencing artefact ID and required action.

technology:
    - title: GitHub Actions / Scheduled Pipeline
      content: |
        Scans front matter dates; raises issues or updates badges.
    - title: Metadata Linter Script
      content: |
        Validates required fields (owner_role, last_verified) exist.
    - title: Dashboard (Grafana / Superset)
      content: |
        Visualises freshness metrics sourced from repository JSON export.
    - title: Calendar / Reminder Integration
      content: |
        Optional ICS feed for upcoming review deadlines.

further_reading:
    - title: Docs as Code Patterns
      content: Techniques enabling automated validation & review workflows.
      url: https://www.thoughtworks.com/radar/techniques/docs-as-code
    - title: Architecture Governance Practices
      content: Ensuring currency of design artefacts within governance cycles.
      url: https://martinfowler.com/architecture/
    - title: Continuous Documentation (Tech Radar)
      content: Concept of integrating documentation freshness in pipelines.
      url: https://www.thoughtworks.com/radar

assessment_guidance: |
  How to assess:
    1. Request freshness report or generate one: list artefacts with last_verified and next_review_due.
    2. Randomly select 5 artefacts: confirm content is still accurate (spot outdated diagrams, retired components, stale decisions).
    3. Identify % artefacts overdue. If >10–15%, probe causes (capacity, ownership, process gaps).
    4. Check automation: is there a script / job creating reminders or status badges? If manual spreadsheet → risk.
    5. Confirm archived / superseded items clearly marked (not deleted) to preserve rationale history.

  Evidence:
    - Freshness dashboard or CSV export.
    - PRs / commits performing review-only updates.
    - Overdue artefact list + remediation plan.
    - Metadata schema (fields documented & enforced).

  Red flags:
    - “Last updated” used instead of explicit verification date.
    - Bulk mass‑edit with meaningless “tidy” commit messages.
    - Review dates all identical (tick‑box exercise).

  Target indicators:
    - ≥85% artefacts within SLA.
    - Mean overdue days trending downward.
    - Automated job output accessible to all stakeholders.

  Quick wins:
    - Add last_verified / next_review_due front matter & simple Ruby script to flag overdue.
    - Introduce a CHANGELOG entry for review-only passes (improves visibility).
    - Badge or colour code stale items in generated site.
    
assessment_examples:
  '0':
    - example: No freshness tracking; obviously obsolete docs (retired components) still primary reference.
    - example: Superseded decisions overwritten or deleted losing rationale.
  '1':
    - example: Manual spot reviews; dates not recorded reliably.
    - example: Spreadsheet lists some artefacts; many missing / stale rows.
  '2':
    - example: Freshness metadata present for critical artefacts only; coverage <50%.
    - example: Reminder process manual; backlog of overdue items grows.
  '3':
    - example: 70–80% coverage with last_verified & next_review_due; overdue list tracked.
    - example: Dashboard/report produced periodically; remediation tickets raised.
  '4':
    - example: ≥85% artefacts in-date; overdue items all have owners & action dates.
    - example: Automated job updates freshness badges & opens issues for breaches.
  '5':
    - example: "≥95% critical artefacts within SLA; stale items rare & short-lived."
    - example: Freshness gating influences governance decisions; metrics show sustained improvement.
    - example: Predictive scheduling anticipates heavy review periods.






---
